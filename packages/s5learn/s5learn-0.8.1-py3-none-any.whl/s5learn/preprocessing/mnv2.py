def mnv2():

    print("""
    
    ## Libraries
import numpy as np
import mindspore as ms
import mindspore.nn as nn
import mindspore.ops as ops


## Make Divisible
def _make_divisible(v, divisor, min_value=None):
	if min_value is None:
		min_value = divisor
 	new_v = max(min_value, int(v + divisor / 2) // divisor * divisor)
 	# Make sure that round down does not go down by more than 10%.
 	if new_v < 0.9 * v:
 		new_v += divisor
 	return new_v



class GlobalAvgPooling(nn.Cell):
	def __init__(self):
		super(GlobalAvgPooling, self).__init__()
		self.mean = ops.ReduceMean(keep_dims=False)


	def construct(self, x):
		x = self.mean(x, (2, 3))
		return x



class ConvBNReLU(nn.Cell):
	def __init__(self, in_planes, out_planes, kernel_size=3, stride=1, groups=1):
 		super(ConvBNReLU, self).__init__()
		padding = (kernel_size - 1) // 2
		in_channels = in_planes
		out_channels = out_planes
		
		if groups == 1:
			conv = nn.Conv2d(in_channels, out_channels, kernel_size, stride, pad_mode='pad', padding=padding)
		else:
			out_channels = in_planes
			conv = nn.Conv2d(in_channels, out_channels, kernel_size, stride, pad_mode='pad',  padding=padding, group=in_channels)
	
		layers = [conv, nn.BatchNorm2d(out_planes), nn.ReLU6()]
		self.features = nn.SequentialCell(layers)
	def construct(self, x):
		output = self.features(x)
		return output



class InvertedResidual(nn.Cell):
	def __init__(self, inp, oup, stride, expand_ratio):
		super(InvertedResidual, self).__init__()
		assert stride in [1, 2]
		hidden_dim = int(round(inp * expand_ratio))
		self.use_res_connect = stride == 1 and inp == oup
		layers = []

		if expand_ratio != 1:
			layers.append(ConvBNReLU(inp, hidden_dim, kernel_size=1))

		layers.extend([
			# dw
			ConvBNReLU(hidden_dim, hidden_dim,
			stride=stride, groups=hidden_dim),

			# pw-linear
			nn.Conv2d(hidden_dim, oup, kernel_size=1, stride=1, has_bias=False),
			nn.BatchNorm2d(oup)])


 		self.conv = nn.SequentialCell(layers)
 		self.add = ops.Add()
 		self.cast = ops.Cast()

 	def construct(self, x):
		identity = x
		x = self.conv(x)
		if self.use_res_connect:
			return self.add(identity, x)
		return x





class MobileNetV2Backbone(nn.Cell):
	def __init__(self, width_mult=1., inverted_residual_setting=None, round_nearest=8,
		input_channel=32, last_channel=1280):
		super(MobileNetV2Backbone, self).__init__()
		block = InvertedResidual
 
		# setting of inverted residual blocks
 		self.cfgs = inverted_residual_setting

 		if inverted_residual_setting is None:
 			self.cfgs = [
				 # t, c, n, s
			 	[1, 16, 1, 1],
 				[6, 24, 2, 2],
 				[6, 32, 3, 2],
 				[6, 64, 4, 2],
 				[6, 96, 3, 1],
 				[6, 160, 3, 2],
 				[6, 320, 1, 1],
 					]
 		# building first layer
		input_channel = _make_divisible(input_channel * width_mult, round_nearest)
		self.out_channels = _make_divisible(last_channel * max(1.0, width_mult), round_nearest)

		features = [ConvBNReLU(3, input_channel, stride=2)]
		# building inverted residual blocks

		for t, c, n, s in self.cfgs:
			output_channel = _make_divisible(c * width_mult, round_nearest)

			for i in range(n):
				stride = s if i == 0 else 1
				features.append(block(input_channel, output_channel, stride, expand_ratio=t))
				input_channel = output_channel

 		# building last several layers
 		features.append(ConvBNReLU(input_channel, self.out_channels, kernel_size=1))
 		# make it nn.CellList
 		self.features = nn.SequentialCell(features)
		self._initialize_weights()

	def construct(self, x):
		x = self.features(x)
		return x

	def _initialize_weights(self):
		self.init_parameters_data()

		for _, m in self.cells_and_names():
			if isinstance(m, nn.Conv2d):
				n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels
				m.weight.set_data(ms.Tensor(np.random.normal(0, np.sqrt(2. / n), m.weight.data.shape).astype("float32")))

			if m.bias is not None:
 				m.bias.set_data(ms.numpy.zeros(m.bias.data.shape, dtype="float32"))
			elif isinstance(m, nn.BatchNorm2d):
 				m.gamma.set_data(ms.Tensor(np.ones(m.gamma.data.shape, dtype="float32")))
 				m.beta.set_data(ms.numpy.zeros(m.beta.data.shape, dtype="float32"))

	@property
	def get_features(self):
		return self.features



class MobileNetV2Head(nn.Cell):
	def __init__(self, input_channel=1280, num_classes=1000, has_dropout=False, activation="None"):
		super(MobileNetV2Head, self).__init__()
		# mobilenet head
		head = ([GlobalAvgPooling()] if not has_dropout else [GlobalAvgPooling(), nn.Dropout(0.2)])
		self.head = nn.SequentialCell(head)
		self.dense = nn.Dense(input_channel, num_classes, has_bias=True)
		self.need_activation = True	
		if activation == "Sigmoid":
			self.activation = ops.Sigmoid()
		elif activation == "Softmax":
 			self.activation = ops.Softmax()
		else:
			self.need_activation = False
		self._initialize_weights()

	def construct(self, x):
		x = self.head(x)
		x = self.dense(x)

		if self.need_activation:
 			x = self.activation(x)
		return x

	def _initialize_weights(self):
		self.init_parameters_data()
 		for _, m in self.cells_and_names():
 			if isinstance(m, nn.Dense):
				m.weight.set_data(ms.Tensor(np.random.normal(0, 0.01, m.weight.data.shape).astype("float32")))
 				if m.bias is not None:
					m.bias.set_data(ms.numpy.zeros(m.bias.data.shape, dtype="float32"))


class MobileNetV2Combine(nn.Cell):
	def __init__(self, backbone, head):
 		super(MobileNetV2Combine, self).__init__(auto_prefix=False)
 		self.backbone = backbone
		self.head = head
	def construct(self, x):
		x = self.backbone(x)
		x = self.head(x)
		return x

def mobilenet_v2(num_classes):
	backbone_net = MobileNetV2Backbone()
	head_net = MobileNetV2Head(backbone_net.out_channels,num_classes)
	return MobileNetV2Combine(backbone_net, head_net)
    
    """)