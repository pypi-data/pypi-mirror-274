# RAG-Py

# Introduction
The project mainly aims to  simulates the creating  of the RAGPY system.
 RAGPY is a theoretical framework that aims to improve the quality and relevance of responses generated by AI systems through a combination of retrieval and generation techniques. By leveraging retrieved information and context,  it performs the retrieval benchmarking . It enhances the generative process to produce more accurate, coherent, and contextually relevant responses.

## Prerequisites
Python 3.10

## Installation

1. Clone the repository:  
   ```bash
   ! git clone https://github.com/bayeslabs/Ragpy.git
   cd ./Ragpy
   ```

2. Install the required packages:
   ```bash
   ! pip install -e .
   ```

## Usage

Run the script main.py with  command-line argument
```!python main.py -h```
 By executing the RAG pipeline. Here are the available options that you can customise with the parsers
```
--config: Path to the configuration file (default: ./config.yaml)
--user_files: Path(s) to user-specified file(s) to be processed
--chunk_size: Chunk size for splitting text (default: 400)
--text_overlap: Text overlap for splitting text (default: 50)
--embedding: List of embedding options (e.g., huggingface_instruct_embeddings, all_minilm_embeddings, etc.)
--vectorstore: Vector store option (Chroma or Faiss)
--persist_dir: Path to the vector store persistent directory
--top_k: Number of top documents to be retrieved
--benchmark_data_path: Path to the benchmarking dataset in CSV with query context and ground_truth
--save_dir: Directory to save all the results
--num_questions: Number of questions to be generated in synthetic benchmarking dataset
--query: The query for which the main logic is executed
--context_given: Whether or not the context is given
--model_type: The type of model to use (openai or hugging_face)
--chain_type: The type of chain to use (simple or retrieval)
--domain: The domain for which main logic needs to be executed
--prompt_type: The type of prompt to use (general, custom, or specific)
--temperature: Temperature of the model (default: 0.7)
--llm_repo_id: Hugging face repo ID for language modelling
--db_path: Path of the database
```
 
## Example
```bash
python main.py --config ./config.yaml --user_files /path/to/files --chunk_size 400 --top_k 5
```

## Executing RAGPY

Ensure you have the necessary API keys set up:
- `OPENAI_API_KEY`: Your OpenAI API key
- `HUGGINGFACEHUB_API_TOKEN`: Your Hugging Face Hub API token

Run RAGPY with the desired configuration:
```bash
python main.py --config path/to/config.yaml
```

## Configuration
Customize the behavior of RAGpy using the `config.yaml` file. Refer to the configuration file for detailed options and descriptions.
