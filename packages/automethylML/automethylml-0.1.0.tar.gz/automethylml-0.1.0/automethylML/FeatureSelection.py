import warnings
import numpy as np
import pandas as pd
import scipy.stats


"""
#$%^&*
to do, all user to pass un a save directory and save base name

set pickle save to utils 

set data time init and set version if there is another pipeliune data file, 
this must be done at the beginning to make sure that the user doesnt run everything 
and then either delete old file or doesnt save the new one. make sure default pcikel behavior is to 
NOT overwrite data
"""



class FeatureSelection():
    def __init__(self, feature_keys, hierarchy_keys, hierarchy_levels=None, index_key_name = None, select_top_n_features=30000):

        """
        A class for performing feature selection based on a hierarchical structure of features.
    
        This class supports the identification and selection of top N features based on the interquartile range (IQR),
        within a  given hierarchical context (e.g., superfamily, family, class).
    
        Parameters:
        - hierarchy_keys (List[str]): Keys representing the hierarchical levels (e.g., ['Superfamily_code', 'Family_code', 'Class_v12_code']).
        - hierarchy_levels (List[int]): Numeric representation of each level's hierarchy, where lower numbers indicate higher hierarchy levels.
        - feature_keys (List[str]): List of strings representing the features to be considered for selection.
        - select_top_n_features (int): Number of top features to select based on IQR. Set to None to ignore IQR and define a structure to use with RF 
    
        Attributes:
        - feature_dict (dict): A dictionary to store the selected top N features across different hierarchical levels.
        - ordered_hierarchy_keys (List[str]): Sorted list of hierarchy keys based on their levels.
        - feature_keys (List[str]): See parameters.
        - select_top_n_features (int): See parameters.
        """


        if isinstance(hierarchy_keys, str):
            hierarchy_keys = [hierarchy_keys]  # Convert to list if string is passed in 
        if len(hierarchy_keys) == 1:
            hierarchy_levels = [0] # special case where there is no hierarchy so we define this to ensure the system works

        assert len(hierarchy_keys) == len(hierarchy_levels), "Length of hierarchy_keys and hierarchy_levels must be equal"
        
        if not all(isinstance(item, str) for item in hierarchy_keys):
            raise TypeError("hierarchy_keys must be a list of strings.")
        if not all(isinstance(item, int) for item in hierarchy_levels):
            raise TypeError("hierarchy_levels must be a list of integers.")

        #idx to DataFrame
        self.selected_feature_dict = {} # for each sub model which features are selected?
        self.sample_index_dict = {} # for each sub model which samples apply?
        self.target_key_dict = {} # for each sub model what targets are we predicting? 

        self.count_per_target = {} # for each sub model what is the count of each target?
        self.max_cross_validation_splits = {} # for each sub model what is the max number of CV splits we can do?
        self.remove_samples_index = {} # for each sub model which samples are removed because there are not enough samples for CV
        
        self.one_outcome_mapping_dict = {} # for each node where only one possible outcome can occur, indicate it here, we don't need a model
        
        self.ordered_hierarchy_keys = [key for _, key in sorted(zip(hierarchy_levels, hierarchy_keys))]
        self.n_levels = len(hierarchy_levels)
        
        self.feature_keys = feature_keys
        self.select_top_n_features = select_top_n_features
        self.index_key_name = index_key_name

    def get_hierarchical_every_outcome_dict(self): 
        #$%^& i think i can just take the unique of each of the DF columns of target LOL --
        # but i guess this does standardize it to an existing order that we have.
        hierarchical_every_outcome_dict = {}
        
        # {'Superfamily_code':  'Full_model',
        #  'Family_code':       'Superfamily_code',
        #  'Class_v12_code':    'Family_code'}
        for hierarchy_key_name, model_key_name in self.hierarchy_key_to_level_map.items():
            x = [vv for kk, vv in self.classification_tree_classes.items() if model_key_name in kk]
            hierarchical_every_outcome_dict[hierarchy_key_name] = (np.concatenate(x))
        
        self.hierarchical_every_outcome_dict = hierarchical_every_outcome_dict

        
    def create_pipeline_data(self):
        """
        Extracts important data from the instance and compiles it into a dictionary. used to make the 
        final model save with all important information. 
        
        This method consolidates various types of data, including dictionaries, lists,
        and integers, collected or generated by the class instance into a single output dictionary.
        
        Returns:
            dict: A dictionary containing selected feature information, sample data,
                  target keys, hierarchy information, feature keys, and configuration integers.
        """
        
        
        out_dict = {
            # these apply to the entire dataset
            'ALL_sample_index': self.ALL_sample_index, # bool of all true with the matching unique ID index
            'ALL_feature_keys': self.feature_keys, #list of all keys to the feature (names of the feature columns)
            'index_key_name': self.index_key_name, # name of the index in the dataframe, unique IDs 
            'ALL_global_keys': self.ALL_global_keys, # keys to each category of the hierarchy

            # these are broken up by each sub model (ALL_global_keys are all the keys to these) 
            'IQR_selected_feature_dict': self.selected_feature_dict, # selected features for each sub model
            'sample_index_dict': self.sample_index_dict, # index to the samples for each sub model NOTE this is AFTER the one sample cases are removed via remove_samples_index
            'target_key_dict': self.target_key_dict, # index to the hierarchy target key for each sub model 
            'count_per_target':self.count_per_target, # count of each target for each sub model
            'max_cross_validation_splits':self.max_cross_validation_splits, # for each sub model what is the max number of CV splits we can do?
            'remove_samples_index':self.remove_samples_index, # for each sub model which samples are removed because there are not enough samples for CV


            # has ALL_global_keys AND condition where there is only one condition so the full tree structure
            'classification_tree_classes':self.classification_tree_classes,

            # length of 'n_hierarchy_levels' where each contains all the possibel classifications at the hierarchy
            'hierarchical_every_outcome_dict':self.hierarchical_every_outcome_dict,

            # general info 
            'ordered_hierarchy_keys': self.ordered_hierarchy_keys, 
            'n_hierarchy_levels': self.n_levels, # int: len() of ordered_hierarchy_keys
            'IQR_select_top_n_features': self.select_top_n_features, # int: number of features selected for IQR feature selection
            'hierarchy_key_to_level_map': self.hierarchy_key_to_level_map,
            
            # for inference when we have only one option for a particular "classification" problem, this maps that to the correct output
            'one_outcome_mapping_dict': self.one_outcome_mapping_dict,
            
        }
        return out_dict

    @staticmethod   
    def _calculate_IQR(df, chunk_size=10000):
        """
        Calculate the IQR for each column in a large DataFrame using scipy's iqr function,
        while processing in chunks to manage memory usage.
    
        Args:
        df (DataFrame): The DataFrame to process.
        chunk_size (int): The number of columns to process in each chunk.
    
        Returns:
        Series: A Series containing the IQR for each column, indexed by column names.
        """
        iqr_results = []
        num_chunks = int(np.ceil(df.shape[1] / chunk_size))
    
        for i in range(num_chunks):
            start_col = i * chunk_size
            end_col = min(start_col + chunk_size, df.shape[1])
            # Apply IQR directly on the sliced DataFrame
            iqr_chunk = scipy.stats.iqr(df.iloc[:, start_col:end_col], axis=0, nan_policy='omit')
            iqr_results.extend(iqr_chunk)
    
        # Create a Series from the results with column names as indices
        column_names = df.columns.tolist()
        iqr_full = pd.Series(data=iqr_results, index=column_names[:len(iqr_results)])
    
        return iqr_full


        

    @staticmethod  
    def _get_top_n_IQR_index(IQR, N):
        """
        Selects the top N indices from a pandas Series based on the Interquartile Range (IQR) values.
    
        Parameters:
        - IQR (pd.Series): A pandas Series containing the interquartile ranges of features.
        - N (int, optional): The number of top indices to select based on the highest IQR values.
                             If N is None, all indices are selected, which is useful for initializing
                             the dictionary without filtering by IQR but requiring all valid model
                             node points (where there is more than one class at the model node).
    
        Returns:
        - pd.Index: An index object containing the top N indices from the IQR Series. If N is None,
                    returns the indices for all entries in the IQR Series.
    
        Raises:
        - TypeError: If the input `IQR` is not a pandas Series.
        """
        if N > len(IQR):
            raise ValueError("selects_top_n_features cannot be greater than the number of elements the DataFrame.")
        if not isinstance(IQR, pd.Series):
            raise TypeError("Input must be a pandas Series.")
        if N is None:
            N = len(IQR)
        return IQR.nlargest(N).index        
    
    def make_feature_selection_dict(self, df): #$% save IQR for all features?? no doesnt make sense, it is easy to calc on the spot when needed
        """
        Constructs a dictionary of selected features based on their IQR, across different hierarchical levels of the given DataFrame.
    
        Parameters:
        - df (pd.DataFrame): The DataFrame containing the features and hierarchical information.
    
        This method populates the `feature_dict` attribute with the indices of the top N features, selected based on their IQR, for each hierarchical level and category. "All_samples" is for the highest level model across all features. Other features are automatically categorized by the names in the dataframe.
        """

        assert isinstance(df, pd.DataFrame), "df must be a pandas DataFrame"

        # REVIEW - test to make sure this works with None, working index and not working index 
        if self.index_key_name is not None and self.index_key_name not in df.index.names:
            raise ValueError(f"Index key name {self.index_key_name} not found in DataFrame index names. Ensure you set the dataframe index to match the 'index_key_name' input variable")

        # special case where we use all features for Full model
        IQR, top_n_features_index, selected_rows_index = self._select_features(df, None, None) # None indicated  this special case for Full model
        self.selected_feature_dict['Full_model'] = top_n_features_index
        self.sample_index_dict['Full_model'] = selected_rows_index
        
        # save for the pipeline dictionary save 
        self.ALL_sample_index = selected_rows_index

        # all but last one, we arent training models on a single class (end of tree)
        for hierarchy_key in self.ordered_hierarchy_keys[:-1]: # e.g. 'Superfamily_code', 'Family_code', 'Class_v12_code'
            for hierarchy_name in np.unique(df[hierarchy_key]):# e.g. the unique names under 'Superfamily_code' or 'Family_code etc
                print(f"Getting IQR for ({hierarchy_key}, {hierarchy_name})") # global_key is the same as (hierarchy_key, hierarchy_name)
                IQR, top_n_features_index, selected_rows_index = self._select_features(df, hierarchy_key, hierarchy_name)
                if IQR is not None: # only create if sub class has more than one possible outcome
                    self.selected_feature_dict[(hierarchy_key, hierarchy_name)] = top_n_features_index
                    self.sample_index_dict[(hierarchy_key, hierarchy_name)] = selected_rows_index

        # define the target keys for future reference. used to find the next step in the hierarchy
        for key in self.selected_feature_dict:
            # for full model use all rows/samples for other models use the name to find the next to the right in the hierarchy order 
            target_key = [self.ordered_hierarchy_keys[0] if key == 'Full_model' else self._next_level(key[0])]
            self.target_key_dict[key] = target_key


        # any of the dictionaries will work because they all have the same global keys
        # for consistnacy I am making a list here of the global keys to reference
        self.ALL_global_keys = [k for k in self.sample_index_dict.keys()]

        self.classification_tree_classes = self.get_classification_tree_classes(self.ALL_global_keys, df)

        
        # #$%^ may need to rename this but use useful when pooling hearchy of prediciton e.g . the 'Full_model'
        # applies to all predictions at the 'Superfamily_code' level, and Superfamily_code applies to 'Family_code'
        # etc so this is just useful and clean to define here once
        hierarchy_map_val = ["Full_model"] + self.ordered_hierarchy_keys[:-1]
        self.hierarchy_key_to_level_map = {k:v for k, v in zip(self.ordered_hierarchy_keys, hierarchy_map_val)}

        self.get_hierarchical_every_outcome_dict() # for each full level these are all the set ordered groups of classes


    

    def _test_if_only_one_outcome(self, hierarchy_key, hierarchy_name, subset_df):
        """
        Tests if a given hierarchical level and name combination in the subset DataFrame has only one possible outcome in the next hierarchical level.
    
        Parameters:
        - hierarchy_key (str): The current hierarchical level's key.
        - hierarchy_name (str): The name of the current category within the hierarchical level.
        - subset_df (pd.DataFrame): The subset DataFrame filtered for the given hierarchy_key and hierarchy_name.
    
        Returns:
        - (bool, str): A tuple where the first element indicates whether only one outcome exists (True if only one, False otherwise),
          and the second element is a message detailing the result.
        """
        # TEST IF ONLY ONE POSSIBLE OUTCOME. 
        # In some cases a level 'P' hierarchy will have only one possible solution for level 'P+1' 
        # e.g. Superfamily_code == 'MENING' only has Family_code == 'MENINGI' so we will not build a model for  
        # MENING to predict the Family_code since their is only one. next_hierarchy_key is here becuase we want 
        # to allow for the edge case of if for example Family_code == 'MENINGI' might have more than one possible 
        # class for example SF(1), F(1), C(2), there the "()" represents how many nodes in a downstream tree. In 
        # this case we would not build a model at SF to predict F, but we WOULD build one at F to predict C. So 
        # next_hierarchy_key represents 'P+1'.
        
        next_hierarchy_key = self._next_level(hierarchy_key)
        
        index_to_test = [hierarchy_key, next_hierarchy_key] # P and P+1 index
        unique_subset_df = subset_df[index_to_test].reset_index(drop=True)
        test_only_one_option = (unique_subset_df.nunique() == 1).all() # if true don't build a model, b/c only one option

        return_msg = (
            f'SKIPPING... {hierarchy_key:20} {hierarchy_name:20} '
            f'because it only has one {next_hierarchy_key:20} {unique_subset_df[next_hierarchy_key][0]:20}'
        )
        if test_only_one_option:
            # for prediction we need a mapping for these special cases
            one_outcome_mapping = {(hierarchy_key, hierarchy_name): (next_hierarchy_key, unique_subset_df[next_hierarchy_key][0])}
        else:
            one_outcome_mapping = None


        return test_only_one_option, return_msg, one_outcome_mapping
        
    def _test_if_hierarchy_key_in_ordered_hierarchy_keys(self, hierarchy_key):
        """
        Simple test to check if hierarchy_key is in self.ordered_hierarchy_keys
        """
        if hierarchy_key not in self.ordered_hierarchy_keys:
            raise ValueError("hierarchy_key is not in the list of ordered hierarchy keys.")
    
    
    def _next_level(self, hierarchy_key):
        """
        finds the next level based on input (hierarchy_key), e.g. input "Superfamily_code" and it returns "Family_code". More generally, if input is self.ordered_hierarchy_keys[0] it returns self.ordered_hierarchy_keys[1]

        Parameters:
        -hierarchy_key (str): string contained in self.ordered_hierarchy_keys

        Returns:
        - (str): of previous level in self.ordered_hierarchy_keys

        Raises:
        - ValueError: If the input hierarchy_key is not found in self.ordered_hierarchy_keys.
        - IndexError: If the input hierarchy_key is the last element in self.ordered_hierarchy_keys, indicating there is no next level.

        """
        self._test_if_hierarchy_key_in_ordered_hierarchy_keys(hierarchy_key)
        if hierarchy_key == self.ordered_hierarchy_keys[-1]:
            raise IndexError("Input hierarchy_key is the last element in the list; no next level exists.")
            
        next_hierarchy_key = self.ordered_hierarchy_keys[self.ordered_hierarchy_keys.index(hierarchy_key) + 1] # P+1 index
        return next_hierarchy_key

    def _previous_level(self, hierarchy_key):
        """
        finds the previous level based on input (hierarchy_key), e.g. input "Family_code" and it returns "Superfamily_code". More generally, if input is self.ordered_hierarchy_keys[1] it returns self.ordered_hierarchy_keys[0]

        Parameters:
        -hierarchy_key (str): string contained in self.ordered_hierarchy_keys

        Returns:
        - (str): of previous level in self.ordered_hierarchy_keys
        """
        self._test_if_hierarchy_key_in_ordered_hierarchy_keys(hierarchy_key)
        if hierarchy_key == self.ordered_hierarchy_keys[0]:
            raise IndexError("Input hierarchy_key is the first element in the list; no previous level exists.")
            
        previous_hierarchy_key = self.ordered_hierarchy_keys[self.ordered_hierarchy_keys.index(hierarchy_key) - 1] # P-1 index
        return previous_hierarchy_key
    
    @staticmethod
    def warn_based_on_number_of_samples_in_each_class(count_per_target, global_key, warn_if_less_than=8):
        """
        Warns the user if there are insufficient samples for cross-validation splits.
        """
        warning_message = (
        f"\nThe training data for model {global_key} has at least one class with "
        f"less than the recommended minimum of {warn_if_less_than}.\n"
        f"Here are the sample counts per each class:\n{count_per_target}"
        )

        if count_per_target.min()<warn_if_less_than:
            warnings.warn(warning_message)

    def get_max_cross_validation_splits(self, df, hierarchy_key_next, global_key):

        test_enough_samples = self.count_per_target[global_key]>=2 # min CV splits is 2 
        if not test_enough_samples.all():
            insufficient_samples = [key for key, enough_samples in test_enough_samples.items() if not enough_samples]
            remove_samples_index = df[hierarchy_key_next].isin(insufficient_samples)

            warning_message = (
            f"\nRemoving samples from {global_key} training data because there is only "
            f"one sample for the following classes:\n" + "\n".join(insufficient_samples)
            )
            
            warnings.warn(warning_message)
            # warnings.warn(f"Removing samples from {global_key} training data because there is only one sample for the following classes:"+ "\n".join(insufficient_samples))
        else:
            remove_samples_index = pd.Series(False, index=df.index)

        self.max_cross_validation_splits[global_key] = np.max([2, self.count_per_target[global_key].min()]) # min CV splits is 2
        self.remove_samples_index[global_key] = remove_samples_index
        


    def _select_features(self, df, hierarchy_key, hierarchy_name):
        """
        Selects the top N features based on IQR for a specific hierarchy_key and hierarchy_name, or globally if both are None.
    
        Parameters:
        - df (pd.DataFrame): The DataFrame containing the features and hierarchical information.
        - hierarchy_key (str, optional): The hierarchical level's key for which to select features. If None, global feature selection is performed across all samples
        - hierarchy_name (str, optional): The name within the hierarchical level for which to select features.
    
        Returns:
        - (pd.Series, pd.Index): A tuple containing the IQR for the selected features and the indices of the top N features. Returns (None, None) if only one outcome is possible, indicating no need for feature selection.
        """
        if hierarchy_key is None: # special case for when all features (for first model which predicts most major class e.g. super family)
            hierarchy_key_next = self.ordered_hierarchy_keys[0]
            global_key = 'Full_model'
            # This will select all rows and keep in the same format as the rows selected in the else statement
            selected_rows_index = pd.Series(True, index=df.index)
            
        else: #Select the samples (rows) which apply 
            hierarchy_key_next = self._next_level(hierarchy_key)
            global_key = (hierarchy_key, hierarchy_name)
            # get rows that are relivent to the specifical sub model
            selected_rows_index = df[hierarchy_key] == hierarchy_name
        
        # select rows for this hierarchy
        subset_df = df.loc[selected_rows_index]
        # check the count for the target variable and save to self
        self.count_per_target[global_key] = subset_df[hierarchy_key_next].value_counts()
        # create variables max_cross_validation_splits and remove_samples_index
        self.get_max_cross_validation_splits(df, hierarchy_key_next, global_key)

        if self.remove_samples_index[global_key].any():
            print(1)

        # reindex the subset_df to account for any values we need to remove due to only 1 sample in a class
        selected_rows_index[self.remove_samples_index[global_key]] = False
        subset_df = df.loc[selected_rows_index]

        self.warn_based_on_number_of_samples_in_each_class(self.count_per_target[global_key], global_key, warn_if_less_than=8)

        if hierarchy_key is not None: 
            only_one_outcome_test, return_msg, one_outcome_mapping  = self._test_if_only_one_outcome(hierarchy_key, hierarchy_name, subset_df)
        else: # full model special case
            if len(self.count_per_target[global_key])<2: # if only one possible outcome in the class
                raise ValueError("The Full_model (aka the highest level of the hearchy) must have more than one possible outcome in the class and each class must have at least 2 samples.")
            only_one_outcome_test = False

        if only_one_outcome_test:
            print(return_msg)
            self.one_outcome_mapping_dict.update(one_outcome_mapping)
            return None, None, None

        
        IQR = self._calculate_IQR(subset_df[self.feature_keys]) # get IQR of only the features 
        top_n_features_index = self._get_top_n_IQR_index(IQR, self.select_top_n_features) # select to N IQR values

        return IQR, top_n_features_index, selected_rows_index
    
    def get_classification_tree_classes(self, ALL_global_keys, df):
        assert ALL_global_keys[0] == 'Full_model', 'somehow the first key is not "Full_model", talk to the developer of this package'
        # set "Full_model" to be all unique classes in the first hierarchy column 

        Full_model_classes = np.asarray(sorted(df.loc[:, self.ordered_hierarchy_keys[0]].unique()))

        classification_tree_classes = {'Full_model':Full_model_classes}

        # for each model level global key we want all possible classifications 
        for hierarchy_key, hierarchy_name in ALL_global_keys[1:]:
            # get the next key to get all the values for the downstream classes to the global key class
            next_hierarchy_key = self._next_level(hierarchy_key)
        
            all_target_columns = df.loc[:, self.ordered_hierarchy_keys] # all target data 
            idx = all_target_columns.loc[:, hierarchy_key]==hierarchy_name # get idx of where hierarchy_key column has the specific hierarchy_name key
            all_targets_selected_samples = all_target_columns.loc[idx, :]
            
            unique_target_for_one_downstream = all_targets_selected_samples.loc[:, next_hierarchy_key].unique()
            
            classification_tree_classes[(hierarchy_key, hierarchy_name)] = np.asarray(sorted(unique_target_for_one_downstream))

        for global_key, value in self.one_outcome_mapping_dict.items():
            classification_tree_classes[global_key] = np.asarray([value[-1]])

        return classification_tree_classes



