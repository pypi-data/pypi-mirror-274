{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## import library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hugtokencraft import editor\n",
    "from transformers import BertTokenizer\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load a PreTrainedTokenizer\n",
    "First let us load a pretrained bert tokenizer and examine its vocabulary size and maximum token length  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial vocab size: 30522 words\n",
      "Model's maximum token length: 512\n"
     ]
    }
   ],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "initial_vocab_size=len(tokenizer)\n",
    "print(f\"initial vocab size: {initial_vocab_size}\")\n",
    "print(f\"Model's maximum token length: {tokenizer.model_max_length}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Target vocabulary\n",
    "Now let us select a subset of the vocabulary to keep. We are going to keep the top 10 most frequently used words in the original tokenizer in our modified tokenizer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'thyroid', 'inspections', 'sanjay', 'infused', 'collarbone', 'hideout', 'scandals', 'pudding', 'arsenic', 'wreath'}\n",
      "{'nitrate', 'stalks', 'necessitated', 'dowry', 'disgrace', 'salamanca', 'genealogical', 'wince', 'tbs', 'leases'}\n"
     ]
    }
   ],
   "source": [
    "target_vocab_size=20\n",
    "selected_20_words=editor.get_top_tokens(tokenizer,target_vocab_size)\n",
    "selected_words=set(list(selected_20_words)[:10])\n",
    "selected_words_add=set(list(selected_20_words)[10:])\n",
    "query_text=result = \" \".join(selected_20_words)\n",
    "print(selected_words)\n",
    "print(selected_words_add)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters\n",
    "To get the modified tokenizer, we need two things\n",
    "- Location path to save the tokenizer. This is important as modified tokenizer will not function properly without saving.\n",
    "- New value for model_max_length (Optional). Defaults to None which means no modification. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_directory = os.getcwd()\n",
    "# Define the path where you want to save the tokenizer\n",
    "tokenizer_path = os.path.join(current_directory,\"ModifiedTokenizer\")\n",
    "model_max_length=128"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reduce the vocabulary\n",
    "This is done in 4 steps.\n",
    "- Reduce the vocabulary \n",
    "- Save the modified tokenizer\n",
    "- Load the modified tokenizer for use\n",
    "- A validation check of the integrity of the new tokenizer (autometically done during loading) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving vocabulary to c:\\Users\\fahim\\Documents\\My Python Public\\HugTokenCraft\\ModifiedTokenizer\\vocab.txt: vocabulary indices are not consecutive. Please check that the vocabulary is not corrupted!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving vocabulary to c:\\Users\\fahim\\Documents\\My Python Public\\HugTokenCraft\\ModifiedTokenizer\\vocab.txt: vocabulary indices are not consecutive. Please check that the vocabulary is not corrupted!\n",
      "Saving vocabulary to c:\\Users\\fahim\\Documents\\My Python Public\\HugTokenCraft\\ModifiedTokenizer\\vocab.txt: vocabulary indices are not consecutive. Please check that the vocabulary is not corrupted!\n",
      "Saving vocabulary to c:\\Users\\fahim\\Documents\\My Python Public\\HugTokenCraft\\ModifiedTokenizer\\vocab.txt: vocabulary indices are not consecutive. Please check that the vocabulary is not corrupted!\n",
      "Saving vocabulary to c:\\Users\\fahim\\Documents\\My Python Public\\HugTokenCraft\\ModifiedTokenizer\\vocab.txt: vocabulary indices are not consecutive. Please check that the vocabulary is not corrupted!\n",
      "Saving vocabulary to c:\\Users\\fahim\\Documents\\My Python Public\\HugTokenCraft\\ModifiedTokenizer\\vocab.txt: vocabulary indices are not consecutive. Please check that the vocabulary is not corrupted!\n",
      "Saving vocabulary to c:\\Users\\fahim\\Documents\\My Python Public\\HugTokenCraft\\ModifiedTokenizer\\vocab.txt: vocabulary indices are not consecutive. Please check that the vocabulary is not corrupted!\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Vocabulary reduction: Done\n",
      " Original vocabulary size: 30522\n",
      " New vocabulary size: 15\n",
      "\n",
      "Tokenizer files created\n",
      "\n",
      "Vocabulary was reduced. Requires json editing\n",
      "Tokenizer loaded\n",
      " Starting to edit the json files...\n",
      " Target Special token mapping:\n",
      "  Special Token: [PAD], Token ID: 0\n",
      "  Special Token: [UNK], Token ID: 1\n",
      "  Special Token: [CLS], Token ID: 2\n",
      "  Special Token: [SEP], Token ID: 3\n",
      "  Special Token: [MASK], Token ID: 4\n",
      "\n",
      " 1. Editing added_tokens.json\n",
      " 2. Editing tokenizer_config.json\n",
      " 3. Model max length updated: 128\n",
      " Editing done. \n",
      "Tokenization model creation completed.\n",
      "\n",
      "Tokenization model Validation: Passed\n",
      "\n",
      "15\n"
     ]
    }
   ],
   "source": [
    "modified_tokenizer=editor.reduce_vocabulary(tokenizer,selected_words)\n",
    "tokenizer_path=editor.save_tokenizer(modified_tokenizer,tokenizer_path,model_max_length)\n",
    "modified_tokenizer=editor.load_tokenizer(type(tokenizer),tokenizer_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new vocab size: 15 words\n",
      "New vocabulary:\n",
      "{'[PAD]': 0, '[UNK]': 1, '[CLS]': 2, '[SEP]': 3, '[MASK]': 4, 'wreath': 5, 'hideout': 6, 'inspections': 7, 'sanjay': 8, 'infused': 9, 'pudding': 10, 'arsenic': 11, 'collarbone': 12, 'scandals': 13, 'thyroid': 14}\n",
      "Added words:\n",
      "{'[PAD]': 0, '[UNK]': 1, '[CLS]': 2, '[SEP]': 3, '[MASK]': 4}\n",
      "Tokenized text:\n",
      "['sanjay', 'collarbone', 'scandals', 'pudding', 'hideout', 'arsenic', 'thyroid', 'inspections', 'infused', 'wreath', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]']\n"
     ]
    }
   ],
   "source": [
    "new_vocab_size=len(modified_tokenizer)\n",
    "print(f\"new vocab size: {new_vocab_size}\")\n",
    "print(\"New vocabulary:\")\n",
    "print(modified_tokenizer.get_vocab())\n",
    "print(\"Added tokens:\")\n",
    "print(modified_tokenizer.get_added_vocab())\n",
    "print(\"Tokenized text:\")\n",
    "tokenized_text=modified_tokenizer.tokenize(query_text)\n",
    "print(tokenized_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Expand vocabulary\n",
    "This is done in 4 steps.\n",
    "- Reduce the vocabulary \n",
    "- Save the modified tokenizer\n",
    "- Load the modified tokenizer for use\n",
    "- A validation check of the integrity of the new tokenizer (autometically done during loading) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer files created\n",
      "\n",
      "Vocabulary was expanded. Therefore, skipped json editing\n",
      "\n",
      "Tokenization model Validation: Passed\n",
      "\n",
      "25\n"
     ]
    }
   ],
   "source": [
    "modified_tokenizer=editor.expand_vocabulary(modified_tokenizer,selected_words_add)\n",
    "tokenizer_path=editor.save_tokenizer(modified_tokenizer,tokenizer_path,model_max_length,isreduced=False)\n",
    "modified_tokenizer=editor.load_tokenizer(type(tokenizer),tokenizer_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new vocab size: 25 words\n",
      "New vocabulary:\n",
      "{'[PAD]': 0, '[UNK]': 1, '[CLS]': 2, '[SEP]': 3, '[MASK]': 4, 'tbs': 5, 'wince': 6, 'inspections': 7, 'infused': 8, 'arsenic': 9, 'leases': 10, 'genealogical': 11, 'salamanca': 12, 'thyroid': 13, 'necessitated': 14, 'sanjay': 15, 'collarbone': 16, 'nitrate': 17, 'pudding': 18, 'wreath': 19, 'disgrace': 20, 'hideout': 21, 'stalks': 22, 'dowry': 23, 'scandals': 24}\n",
      "Added words:\n",
      "{'[PAD]': 0, '[UNK]': 1, '[CLS]': 2, '[SEP]': 3, '[MASK]': 4, 'sanjay': 15, 'collarbone': 16, 'nitrate': 17, 'pudding': 18, 'wreath': 19, 'disgrace': 20, 'hideout': 21, 'stalks': 22, 'dowry': 23, 'scandals': 24}\n",
      "Tokenize text:\n",
      "['wince', 'arsenic', 'genealogical', 'necessitated', 'salamanca', 'infused', 'inspections', 'tbs', 'thyroid', 'leases', 'sanjay', 'nitrate', 'collarbone', 'pudding', 'disgrace', 'hideout', 'scandals', 'wreath', 'stalks', 'dowry']\n"
     ]
    }
   ],
   "source": [
    "new_vocab_size=len(modified_tokenizer)\n",
    "print(f\"new vocab size: {new_vocab_size} words\")\n",
    "print(\"New vocabulary:\")\n",
    "print(modified_tokenizer.get_vocab())\n",
    "print(\"Added words:\")\n",
    "print(modified_tokenizer.get_added_vocab())\n",
    "tokenized_text=modified_tokenizer.tokenize(query_text)\n",
    "print(\"Tokenized text:\")\n",
    "print(tokenized_text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pylpc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
