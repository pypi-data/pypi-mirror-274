# -*- coding: utf-8 -*-
from setuptools import setup

packages = \
['aepo', 'aepo.mbr', 'aepo.mbr.policy']

package_data = \
{'': ['*']}

install_requires = \
['datasets>=2.10.0,<3.0.0',
 'evaluate>=0.4.2,<0.5.0',
 'llm-blender>=0.0.2,<0.0.3',
 'numpy>=1.26.4,<2.0.0',
 'pandas>=2.0.0,<3.0.0',
 'torch>=2.3.0,<3.0.0',
 'tqdm>=4.66.4,<5.0.0',
 'transformers>=4.40.2,<5.0.0']

entry_points = \
{'console_scripts': ['aepo = aepo.cli:aepo']}

setup_kwargs = {
    'name': 'aepo',
    'version': '0.1.4',
    'description': 'Annotation Efficient Preference Optimization',
    'long_description': '## Annotation-Efficient Preference Optimization\n\nThis repository implements the Annotation-Efficient Preference Optimization (AEPO) algorithm.\n\nThe code is tested on Ubuntu 20.04 using Python 3.9 and CUDA 11.0 (Docker image nvidia/cuda:11.0.3-cudnn8-devel-ubuntu20.04).\n\n## Install\n\nYou can install aepo via pip.\n```\npip install aepo\n```\n\nSource install is available too. Clone this repository and run `pip install .`.\n```\ngit clone git@github.com:CyberAgentAILab/annotation-efficient-po.git\ncd annotation-efficient-po\npip install .\n```\n\n\n## Usage\n\nThe command line interface is available.\nThe input dataset can be csv file or a dataset uploaded to Huggingface Hub.\nThe dataset should have a column named *prompt* or *instruction*. aepo recognize it as the user prompt given to the system and the rest of the columns to be the responses generated by the system.\n\nI prepared an example dataset in `dataset/alpaca_samples.csv`.\nThe csv file includes 128 responses generated by [HuggingFaceH4/mistral-7b-sft-beta](https://huggingface.co/HuggingFaceH4/mistral-7b-sft-beta) for each instruction of the `alpaca_human_preference` split of [tatsu-lab/alpaca_farm](https://huggingface.co/datasets/tatsu-lab/alpaca_eval).\nYou can try aepo using this dataset with the following command:\n\n```\naepo dataset/alpaca_samples.csv --num_responses 8 --num_annotations 2 --num_instructions 10\n```\n\n`--num_responses` is the number of input responses you use. The dataset has to have responses larger than or equal to `--num_responses`. `--num_annotations` is the number of responses after the subsampling process. It is also the number of times the reward model is queried per instruction.\n\n### Example: Running AEPO\n\nYou can generate a pair of responses for each instruction using aepo using the following command.\n\n```\naepo dataset/alpaca_samples.csv --num_responses 8 --num_annotations 2 --num_instructions 10\n```\n\nTo subsample four responses for e.g., [LiPO](https://arxiv.org/abs/2402.01878v1), set `--num_annotations` to four.\n\n```\naepo dataset/alpaca_samples.csv --num_responses 8 --num_annotations 4 --num_instructions 10\n```\n\n### Example: Running West-of-N over 8 samples\n[West-of-N](https://arxiv.org/abs/2401.12086) is a strategy to pick the Best-of-N as the chosen response, and Worst-of-N as a rejected response. It is shown to be effective for DPO and reward modeling.\nYou can run West-of-N using this package by setting `--num_annotations` == `--num_responses`.\n\n```\naepo dataset/alpaca_samples.csv --num_responses 8 --num_annotations 8 --num_instructions 10\n```\n\nThis command will generate a dataset with 8 responses, ranked by their rewards. If you only need the best and worst of the N samples, then use `--west_of_n` option.\n\n```\naepo dataset/alpaca_samples.csv --num_responses 8 --num_annotations 8 --num_instructions 10 --west_of_n\n```\n\nThis will pick the best and worst responses as the chosen and rejected. The rest of the responses are discarded.\nIt would be useful to construct a pairwise preference dataset.\n\n## Reference\n\nTBA. Yuu Jinnai and Honda Ukyo. Annotation-Efficient Preference Optimization for Language Model Alignment, 2024.\n\n## Contact\nFor any questions, feel free to raise an issue or contact me at jinnai_yu@cyberagent.co.jp.\n',
    'author': 'Yuu Jinnai',
    'author_email': 'ddyuudd@gmail.com',
    'maintainer': None,
    'maintainer_email': None,
    'url': 'https://github.com/CyberAgentAILab/annotation-efficient-po',
    'packages': packages,
    'package_data': package_data,
    'install_requires': install_requires,
    'entry_points': entry_points,
    'python_requires': '>=3.9,<4.0',
}


setup(**setup_kwargs)
