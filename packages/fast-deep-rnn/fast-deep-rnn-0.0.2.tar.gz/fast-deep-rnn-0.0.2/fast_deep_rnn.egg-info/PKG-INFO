Metadata-Version: 2.1
Name: fast-deep-rnn
Version: 0.0.2
Summary: Deep Learning framework extension allowing more efficient backpropogation of gradient in a situation with branched computational graph structure
Home-page: https://github.com/Hacker1337/fast-deep-rnn
Author: Hacker1337
Author-email: amirfvb@gmail.com
License: Apache Software License 2.0
Keywords: nbdev jupyter notebook python
Classifier: Development Status :: 4 - Beta
Classifier: Intended Audience :: Developers
Classifier: Natural Language :: English
Classifier: Programming Language :: Python :: 3.7
Classifier: Programming Language :: Python :: 3.8
Classifier: Programming Language :: Python :: 3.9
Classifier: Programming Language :: Python :: 3.10
Classifier: License :: OSI Approved :: Apache Software License
Requires-Python: >=3.7
Description-Content-Type: text/markdown
License-File: LICENSE
Requires-Dist: ipykernel
Requires-Dist: numpy
Requires-Dist: pandas
Requires-Dist: matplotlib
Requires-Dist: scikit-learn
Requires-Dist: lets_plot
Provides-Extra: dev
Requires-Dist: nbdev; extra == "dev"

# fast-deep-rnn


<!-- WARNING: THIS FILE WAS AUTOGENERATED! DO NOT EDIT! -->

This is the Course Project for the DeepLearning University Course.

## Install

Library can be installed from the PyPI via

``` sh
pip install fast_deep_rnn
```

## Structure

`core` module contains the original core of the framework with Tensor
class implementation and the set of differentiable operations, organized
in Modules.

`core_v2` module contains the alternative proposed implementation,
resulting in much faster gradient computing in RNN-s.

Notebook `nbs/02_minimal_training.ipynb` contains the simplest example
of model, having exponential growth in original gradient computing, and
benchmarking function to measure this growth. git tags
`baseline_benchmark_results` and `solution_benchmark_results` contain
corresponding benchmark results inside the notebook.

Notebook `nbs/01_lstm_training.ipynb` contains training of LSTM on
number sorting task, which became possible only after the implemented
optimization.
